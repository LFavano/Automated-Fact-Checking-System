{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logically - Luca Favano - Delivery.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObR0nXFEjeWy",
        "colab_type": "text"
      },
      "source": [
        "Fact Checking system using Bert.<br>\n",
        "To test the system just run all the cells :) (This version takes approximately 1 hour to run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x_3bRjaFPF8",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvnfgaedFX9m",
        "colab_type": "code",
        "outputId": "c1921c7b-ba57-4afc-9970-a20d391f6755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "source": [
        "# Necessary to install on Google Colab, outside it more will be necessary! (e.g. Tensorflow,Keras)\n",
        "\n",
        "!pip install bert-tensorflow\n",
        "!pip install nltk\n",
        "!pip install google\n",
        "!pip install readability-lxml\n",
        "!pip install h5py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (2.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google) (4.6.3)\n",
            "Collecting readability-lxml\n",
            "  Downloading https://files.pythonhosted.org/packages/af/a7/8ea52b2d3de4a95c3ed8255077618435546386e35af8969744c0fa82d0d6/readability-lxml-0.7.1.tar.gz\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from readability-lxml) (3.0.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from readability-lxml) (4.2.6)\n",
            "Collecting cssselect (from readability-lxml)\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: readability-lxml\n",
            "  Building wheel for readability-lxml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for readability-lxml: filename=readability_lxml-0.7.1-cp36-none-any.whl size=16480 sha256=d1bff3453e0f23f3e2dde7b0c93e508a4a96778d75ea07cc2a3b7086cc4180b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/48/e5/d944e616d8b0734c3b9cf30a21f4afcf855a1e2b85f82f34fb\n",
            "Successfully built readability-lxml\n",
            "Installing collected packages: cssselect, readability-lxml\n",
            "Successfully installed cssselect-1.1.0 readability-lxml-0.7.1\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.16.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHSyQyJMFchi",
        "colab_type": "code",
        "outputId": "4358e7cf-771a-4a92-fd7f-f2a23dcfdc48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from bert.tokenization import FullTokenizer\n",
        "from tqdm import tqdm_notebook\n",
        "from tensorflow.keras import backend as K\n",
        "from googlesearch import search \n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from readability import Document\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "bert_path = \"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\"\n",
        "max_seq_length = 256"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLDA6ZRhRCCl",
        "colab_type": "text"
      },
      "source": [
        "## Bert Tokenization Implementation Code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqojo-1tREQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PaddingInputExample(object):\n",
        "  pass\n",
        "class InputExample(object):\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "    bert_module =  hub.Module(bert_path)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    vocab_file, do_lower_case = sess.run(\n",
        "        [\n",
        "            tokenization_info[\"vocab_file\"],\n",
        "            tokenization_info[\"do_lower_case\"],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        input_ids = [0] * max_seq_length\n",
        "        input_mask = [0] * max_seq_length\n",
        "        segment_ids = [0] * max_seq_length\n",
        "        label = 0\n",
        "        return input_ids, input_mask, segment_ids, label\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
        "\n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    segment_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(0)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    return input_ids, input_mask, segment_ids, example.label\n",
        "\n",
        "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
        "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
        "    for example in examples:\n",
        "        input_id, input_mask, segment_id, label = convert_single_example(\n",
        "            tokenizer, example, max_seq_length\n",
        "        )\n",
        "        input_ids.append(input_id)\n",
        "        input_masks.append(input_mask)\n",
        "        segment_ids.append(segment_id)\n",
        "        labels.append(label)\n",
        "    return (\n",
        "        np.array(input_ids),\n",
        "        np.array(input_masks),\n",
        "        np.array(segment_ids),\n",
        "        #np.array(labels).reshape(-1, 1),\n",
        "        np.array(labels),\n",
        "    )\n",
        "\n",
        "def convert_text_to_examples(texts, labels):\n",
        "    \"\"\"Create InputExamples\"\"\"\n",
        "    InputExamples = []\n",
        "    for text, label in zip(texts, labels):\n",
        "        InputExamples.append(\n",
        "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
        "        )\n",
        "    return InputExamples\n",
        "  \n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "  while True:\n",
        "    total_length = len(tokens_a) + len(tokens_b)\n",
        "    if total_length <= max_length:\n",
        "      break\n",
        "    if len(tokens_a) > len(tokens_b):\n",
        "      tokens_a.pop()\n",
        "    else:\n",
        "      tokens_b.pop()  \n",
        "  \n",
        "def convert_single_example_m(tokenizer, example, max_seq_length=256):\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        input_ids = [0] * max_seq_length\n",
        "        input_mask = [0] * max_seq_length\n",
        "        segment_ids = [0] * max_seq_length\n",
        "        label = 0\n",
        "        return input_ids, input_mask, segment_ids, label\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "    tokens_b = tokenizer.tokenize(example.text_b)\n",
        "      \n",
        "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)    \n",
        "\n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    segment_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(0)\n",
        "    \n",
        "    for token in tokens_b:\n",
        "      tokens.append(token)\n",
        "      segment_ids.append(1)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(1)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    return input_ids, input_mask, segment_ids, example.label\n",
        "\n",
        "def convert_examples_to_features_m(tokenizer, examples, max_seq_length=256):\n",
        "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
        "    for example in examples:\n",
        "        input_id, input_mask, segment_id, label = convert_single_example_m(\n",
        "            tokenizer, example, max_seq_length\n",
        "        )\n",
        "        input_ids.append(input_id)\n",
        "        input_masks.append(input_mask)\n",
        "        segment_ids.append(segment_id)\n",
        "        labels.append(label)\n",
        "    return (\n",
        "        np.array(input_ids),\n",
        "        np.array(input_masks),\n",
        "        np.array(segment_ids),\n",
        "        #np.array(labels).reshape(-1, 1),\n",
        "        np.array(labels),\n",
        "    )\n",
        "\n",
        "def convert_text_to_examples_m(texts,texts2, labels):\n",
        "    InputExamples = []\n",
        "    for text,text2, label in zip(texts,texts2, labels):\n",
        "        InputExamples.append(\n",
        "            InputExample(guid=None, text_a=\" \".join(text), text_b=\" \".join(text2), label=label)\n",
        "        )\n",
        "    return InputExamples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHsB2bWQEhu2",
        "colab_type": "text"
      },
      "source": [
        "## Check Worthiness Dataset and Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFexqtLT38C6",
        "colab_type": "text"
      },
      "source": [
        "Binary Text Classifier that learns to distinguish between what is a claim and what is a fact, claims are considered check-worthy. The dataset is better explained in my paper at http://ceur-ws.org/Vol-2380/paper_119.pdf where the fact that this dataset approximates well the concept of check-worthy is also proved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbwgOxux0LQc",
        "colab_type": "code",
        "outputId": "8884fb79-0a12-4d7f-a102-8c4e647c51b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://github.com/LFavano/utils/raw/master/mega.zip\n",
        "!unzip mega.zip\n",
        "\n",
        "dataset = pd.read_csv('mega.csv', sep='\\t',error_bad_lines=False)\n",
        "\n",
        "dataset['label']=dataset['label'].astype('int8')\n",
        "dataset.info()\n",
        "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "totalRows = 50000\n",
        "trainRatio = 0.8\n",
        "splitIndex = int(trainRatio*totalRows)\n",
        "\n",
        "dataset = dataset[:totalRows]\n",
        "\n",
        "train_df = dataset[:splitIndex]\n",
        "test_df = dataset[splitIndex:]\n",
        "\n",
        "train_df.info()\n",
        "test_df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-28 15:03:11--  https://github.com/LFavano/utils/raw/master/mega.zip\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LFavano/utils/master/mega.zip [following]\n",
            "--2019-09-28 15:03:11--  https://raw.githubusercontent.com/LFavano/utils/master/mega.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75380769 (72M) [application/zip]\n",
            "Saving to: ‘mega.zip’\n",
            "\n",
            "mega.zip            100%[===================>]  71.89M   169MB/s    in 0.4s    \n",
            "\n",
            "2019-09-28 15:03:12 (169 MB/s) - ‘mega.zip’ saved [75380769/75380769]\n",
            "\n",
            "Archive:  mega.zip\n",
            "  inflating: mega.csv                \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2207328 entries, 0 to 2207327\n",
            "Data columns (total 2 columns):\n",
            "headline_text    object\n",
            "label            int8\n",
            "dtypes: int8(1), object(1)\n",
            "memory usage: 18.9+ MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 40000 entries, 0 to 39999\n",
            "Data columns (total 2 columns):\n",
            "headline_text    40000 non-null object\n",
            "label            40000 non-null int8\n",
            "dtypes: int8(1), object(1)\n",
            "memory usage: 351.6+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 40000 to 49999\n",
            "Data columns (total 2 columns):\n",
            "headline_text    10000 non-null object\n",
            "label            10000 non-null int8\n",
            "dtypes: int8(1), object(1)\n",
            "memory usage: 88.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbYbPtOvEyKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create datasets\n",
        "train_text = train_df['headline_text'].tolist()\n",
        "train_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\n",
        "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "train_label = np.asarray(pd.get_dummies(train_df.label), dtype = np.int8)\n",
        "\n",
        "test_text = test_df['headline_text'].tolist()\n",
        "test_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\n",
        "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "test_label = np.asarray(pd.get_dummies(test_df.label), dtype = np.int8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8s5HjEqEziG",
        "colab_type": "code",
        "outputId": "5f098f8d-9c74-4f56-a7dc-3abee372bec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Instantiate tokenizer\n",
        "tokenizer = create_tokenizer_from_hub_module()\n",
        "\n",
        "# Convert data to InputExample format\n",
        "train_examples = convert_text_to_examples(train_text, train_label)\n",
        "test_examples = convert_text_to_examples(test_text, test_label)\n",
        "\n",
        "# Convert to features\n",
        "(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",
        ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
        "(test_input_ids, test_input_masks, test_segment_ids, test_labels\n",
        ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_nOQL0mGHF1",
        "colab_type": "text"
      },
      "source": [
        "## Evidence Detection Dataset and Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AKVn85431c0",
        "colab_type": "text"
      },
      "source": [
        "Binary Text classifier that learn to distinguish between evidences that are supporting a claim versus evidences that are confuting it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A5lHcXKGNYm",
        "colab_type": "code",
        "outputId": "bac279c3-7c0f-4e16-d7c8-f5ebaae5d2db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# An experimental dataset that contains the complete articles too in addition to the claims, extracted from Politifact. I do not own this.\n",
        "!wget https://github.com/LFavano/utils/raw/master/politifact10.zip \n",
        "!unzip politifact10.zip\n",
        "\n",
        "dataset = pd.read_csv('politifact10.csv', sep='\\t',error_bad_lines=False)\n",
        "\n",
        "dataset.info()\n",
        "\n",
        "# Just a way to set as 0 the labels Pants on Fire, False and Mostly False, while as 1 the labels True, Half True, Barely True\n",
        "dataset.label = 5-dataset.label\n",
        "dataset.label[dataset.label.isin([0,1,2])] = 0\n",
        "dataset.label[dataset.label.isin([3,4,5])] = 1\n",
        "\n",
        "train_df = dataset[:(int(trainRatio*len(dataset)))]\n",
        "test_df = dataset[(int(trainRatio*len(dataset))):]\n",
        "\n",
        "train_df.info()\n",
        "test_df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-28 15:03:55--  https://github.com/LFavano/utils/raw/master/politifact10.zip\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LFavano/utils/master/politifact10.zip [following]\n",
            "--2019-09-28 15:03:55--  https://raw.githubusercontent.com/LFavano/utils/master/politifact10.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19220984 (18M) [application/zip]\n",
            "Saving to: ‘politifact10.zip’\n",
            "\n",
            "politifact10.zip    100%[===================>]  18.33M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-09-28 15:03:56 (130 MB/s) - ‘politifact10.zip’ saved [19220984/19220984]\n",
            "\n",
            "Archive:  politifact10.zip\n",
            "  inflating: politifact10.csv        \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10464 entries, 0 to 10463\n",
            "Data columns (total 7 columns):\n",
            "index                      10464 non-null int64\n",
            "Speaker                    10464 non-null object\n",
            "label                      10464 non-null int64\n",
            "claim                      10464 non-null object\n",
            "PolitiFact explanations    10464 non-null object\n",
            "url                        10464 non-null object\n",
            "body                       10464 non-null object\n",
            "dtypes: int64(2), object(5)\n",
            "memory usage: 572.3+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8371 entries, 0 to 8370\n",
            "Data columns (total 7 columns):\n",
            "index                      8371 non-null int64\n",
            "Speaker                    8371 non-null object\n",
            "label                      8371 non-null int64\n",
            "claim                      8371 non-null object\n",
            "PolitiFact explanations    8371 non-null object\n",
            "url                        8371 non-null object\n",
            "body                       8371 non-null object\n",
            "dtypes: int64(2), object(5)\n",
            "memory usage: 457.9+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2093 entries, 8371 to 10463\n",
            "Data columns (total 7 columns):\n",
            "index                      2093 non-null int64\n",
            "Speaker                    2093 non-null object\n",
            "label                      2093 non-null int64\n",
            "claim                      2093 non-null object\n",
            "PolitiFact explanations    2093 non-null object\n",
            "url                        2093 non-null object\n",
            "body                       2093 non-null object\n",
            "dtypes: int64(2), object(5)\n",
            "memory usage: 114.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOhIMij0KsHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create datasets\n",
        "train_text = train_df['claim'].tolist()\n",
        "train_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\n",
        "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "\n",
        "train_text2 = train_df['body'].tolist()\n",
        "train_text2 = [' '.join(t.split()[0:max_seq_length]) for t in train_text2]\n",
        "train_text2 = np.array(train_text2, dtype=object)[:, np.newaxis]\n",
        "\n",
        "train_label = np.asarray(pd.get_dummies(train_df.label), dtype = np.int8)\n",
        "\n",
        "test_text = test_df['claim'].tolist()\n",
        "test_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\n",
        "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "\n",
        "test_text2 = test_df['body'].tolist()\n",
        "test_text2 = [' '.join(t.split()[0:max_seq_length]) for t in test_text2]\n",
        "test_text2 = np.array(test_text2, dtype=object)[:, np.newaxis]\n",
        "\n",
        "test_label = np.asarray(pd.get_dummies(test_df.label), dtype = np.int8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRqAVBc-KveS",
        "colab_type": "code",
        "outputId": "6465cab3-672f-4e19-8e3f-b4803087b324",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Instantiate tokenizer\n",
        "tokenizer = create_tokenizer_from_hub_module()\n",
        "\n",
        "# Convert data to InputExample format\n",
        "train_examples = convert_text_to_examples_m(train_text, train_text2, train_label)\n",
        "test_examples = convert_text_to_examples_m(test_text, test_text2, test_label)\n",
        "\n",
        "# Convert to features\n",
        "(train_input_ids2, train_input_masks2, train_segment_ids2, train_labels2 \n",
        ") = convert_examples_to_features_m(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
        "(test_input_ids2, test_input_masks2, test_segment_ids2, test_labels2\n",
        ") = convert_examples_to_features_m(tokenizer, test_examples, max_seq_length=max_seq_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4q7YCJP5JVL",
        "colab_type": "text"
      },
      "source": [
        "## Relevance Detection Dataset and Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6UyS12CXzEV",
        "colab_type": "text"
      },
      "source": [
        "Very simple text classifier that assign a score to how related the text of the article is with respect to the claim. The dataset is part of the \"FakeNewsChallenge\". http://www.fakenewschallenge.org/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mTJlA-O5Pu1",
        "colab_type": "code",
        "outputId": "f32a9a94-ea1b-4a08-856d-c91ab092a40a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://github.com/LFavano/utils/raw/master/train_bodies.csv\n",
        "!wget https://github.com/LFavano/utils/raw/master/train_stances.csv\n",
        "  \n",
        "_COL_1 = ['Headline', 'Body ID', 'Stance']\n",
        "_COL_2 = ['Body ID', 'articleBody']\n",
        "_COL_3 = ['Headline', 'Body ID', 'Stance', 'articleBody']\n",
        "\n",
        "link = pd.read_csv(\"train_stances.csv\", index_col=None, names=_COL_1, sep=',')\n",
        "bodies = pd.read_csv(\"train_bodies.csv\", index_col=None, names=_COL_2, sep=',')\n",
        "\n",
        "link.Stance[link.Stance=='agree'] = 1\n",
        "link.Stance[link.Stance=='disagree'] = 1\n",
        "link.Stance[link.Stance=='discuss'] = 1\n",
        "link.Stance[link.Stance=='unrelated'] = 0\n",
        "\n",
        "link = link[1:]\n",
        "bodies = bodies [1:]\n",
        "\n",
        "complete = link.merge(bodies, on='Body ID')\n",
        "complete = complete[1:]\n",
        "complete.info()\n",
        "\n",
        "complete = complete.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "train_df = complete[:10000]\n",
        "test_df = complete[10000:12000]\n",
        "\n",
        "train_df.info()\n",
        "test_df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-28 15:04:53--  https://github.com/LFavano/utils/raw/master/train_bodies.csv\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LFavano/utils/master/train_bodies.csv [following]\n",
            "--2019-09-28 15:04:54--  https://raw.githubusercontent.com/LFavano/utils/master/train_bodies.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3752301 (3.6M) [text/plain]\n",
            "Saving to: ‘train_bodies.csv’\n",
            "\n",
            "train_bodies.csv    100%[===================>]   3.58M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2019-09-28 15:04:54 (44.6 MB/s) - ‘train_bodies.csv’ saved [3752301/3752301]\n",
            "\n",
            "--2019-09-28 15:04:55--  https://github.com/LFavano/utils/raw/master/train_stances.csv\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LFavano/utils/master/train_stances.csv [following]\n",
            "--2019-09-28 15:04:55--  https://raw.githubusercontent.com/LFavano/utils/master/train_stances.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4255300 (4.1M) [text/plain]\n",
            "Saving to: ‘train_stances.csv’\n",
            "\n",
            "train_stances.csv   100%[===================>]   4.06M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-09-28 15:04:56 (40.6 MB/s) - ‘train_stances.csv’ saved [4255300/4255300]\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 49971 entries, 1 to 49971\n",
            "Data columns (total 4 columns):\n",
            "Headline       49971 non-null object\n",
            "Body ID        49971 non-null object\n",
            "Stance         49971 non-null object\n",
            "articleBody    49971 non-null object\n",
            "dtypes: object(4)\n",
            "memory usage: 1.9+ MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 4 columns):\n",
            "Headline       10000 non-null object\n",
            "Body ID        10000 non-null object\n",
            "Stance         10000 non-null object\n",
            "articleBody    10000 non-null object\n",
            "dtypes: object(4)\n",
            "memory usage: 312.6+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2000 entries, 10000 to 11999\n",
            "Data columns (total 4 columns):\n",
            "Headline       2000 non-null object\n",
            "Body ID        2000 non-null object\n",
            "Stance         2000 non-null object\n",
            "articleBody    2000 non-null object\n",
            "dtypes: object(4)\n",
            "memory usage: 62.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJXFmgc65x44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create datasets (Only take up to max_seq_length words for memory)\n",
        "train_text = train_df['Headline'].tolist()\n",
        "train_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\n",
        "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "\n",
        "train_text2 = train_df['articleBody'].tolist()\n",
        "train_text2 = [' '.join(t.split()[0:max_seq_length]) for t in train_text2]\n",
        "train_text2 = np.array(train_text2, dtype=object)[:, np.newaxis]\n",
        "\n",
        "train_label = np.asarray(pd.get_dummies(train_df.Stance), dtype = np.int8)\n",
        "\n",
        "test_text = test_df['Headline'].tolist()\n",
        "test_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\n",
        "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "\n",
        "test_text2 = test_df['articleBody'].tolist()\n",
        "test_text2 = [' '.join(t.split()[0:max_seq_length]) for t in test_text2]\n",
        "test_text2 = np.array(test_text2, dtype=object)[:, np.newaxis]\n",
        "\n",
        "test_label = np.asarray(pd.get_dummies(test_df.Stance), dtype = np.int8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIOnOHTA56Wy",
        "colab_type": "code",
        "outputId": "32b3a8b8-a376-4885-8b3f-ce99e895ad6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Instantiate tokenizer\n",
        "tokenizer = create_tokenizer_from_hub_module()\n",
        "\n",
        "# Convert data to InputExample format\n",
        "train_examples = convert_text_to_examples_m(train_text, train_text2, train_label)\n",
        "test_examples = convert_text_to_examples_m(test_text, test_text2, test_label)\n",
        "\n",
        "# Convert to features\n",
        "(train_input_ids3, train_input_masks3, train_segment_ids3, train_labels3\n",
        ") = convert_examples_to_features_m(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
        "(test_input_ids3, test_input_masks3, test_segment_ids3, test_labels3\n",
        ") = convert_examples_to_features_m(tokenizer, test_examples, max_seq_length=max_seq_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yso8XeO5NMqo",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qagIisw43Urh",
        "colab_type": "text"
      },
      "source": [
        "A very simple FFNN model that let us obtain three text classifiers, trained on a small subset of the whole dataset, for a single epoch (highly improvable).\n",
        "\n",
        "The use of GPU is highly suggested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWfMrQvpNN3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        self.output_size = 768\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(\n",
        "            bert_path,\n",
        "            trainable=self.trainable,\n",
        "            name=\"{}_module\".format(self.name)\n",
        "        )\n",
        "        trainable_vars = self.bert.variables\n",
        "        \n",
        "        # Remove unused layers\n",
        "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
        "        \n",
        "        # Select how many layers to fine tune\n",
        "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
        "        \n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "        \n",
        "        # Add non-trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "        \n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "            \"pooled_output\"\n",
        "        ]\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9z6J2NmNQZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a very simple FFNN model\n",
        "def build_model(max_seq_length): \n",
        "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
        "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
        "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
        "    bert_inputs = [in_id, in_mask, in_segment]\n",
        "\n",
        "    bert_output = BertLayer(n_fine_tune_layers=10)(bert_inputs)\n",
        "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
        "    pred = tf.keras.layers.Dense(2, activation='softmax')(dense)\n",
        "    \n",
        "\n",
        "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def initialize_vars(sess):\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PylwsriINiVJ",
        "colab_type": "code",
        "outputId": "3f35d82c-5871-48bd-8a9a-35460df63e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        }
      },
      "source": [
        "modelWorthy = build_model(max_seq_length)\n",
        "\n",
        "initialize_vars(sess)\n",
        "\n",
        "modelWorthy.fit(\n",
        "    [train_input_ids, train_input_masks, train_segment_ids], \n",
        "    train_labels,\n",
        "    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n",
        "    epochs=1,\n",
        "    batch_size=32\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer (BertLayer)          (None, 768)          178565115   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            514         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 178,762,493\n",
            "Trainable params: 6,102,530\n",
            "Non-trainable params: 172,659,963\n",
            "__________________________________________________________________________________________________\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 2031s 51ms/sample - loss: 0.0379 - acc: 0.9875 - val_loss: 0.0179 - val_acc: 0.9952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f40350710f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN0qbXP_Nvfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.models.save_model(modelWorthy,'worthy.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl_JSB0BOU1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del modelWorthy\n",
        "sess.close()\n",
        "K.clear_session()\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZSS7vTGNn2V",
        "colab_type": "code",
        "outputId": "8470535d-45e6-4b2a-ec6e-001f2822520c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "modelEvidences = build_model(max_seq_length)\n",
        "\n",
        "initialize_vars(sess)\n",
        "\n",
        "modelEvidences.fit(\n",
        "    [train_input_ids2, train_input_masks2, train_segment_ids2], \n",
        "    train_labels2,\n",
        "    validation_data=([test_input_ids2, test_input_masks2, test_segment_ids2], test_labels2),\n",
        "    epochs=1,\n",
        "    batch_size=32\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer (BertLayer)          (None, 768)          178565115   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            514         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 178,762,493\n",
            "Trainable params: 6,102,530\n",
            "Non-trainable params: 172,659,963\n",
            "__________________________________________________________________________________________________\n",
            "Train on 8371 samples, validate on 2093 samples\n",
            "8371/8371 [==============================] - 428s 51ms/sample - loss: 0.7247 - acc: 0.5386 - val_loss: 0.6877 - val_acc: 0.5695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f403193ffd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttLSnDuONyDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.models.save_model(modelEvidences,'evidences.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2lPAp176Bqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del modelEvidences\n",
        "sess.close()\n",
        "K.clear_session()\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNBANmEp6D2J",
        "colab_type": "code",
        "outputId": "89a78e5d-09ac-426d-8618-1995d4ffda39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "modelRelevance = build_model(max_seq_length)\n",
        "\n",
        "initialize_vars(sess)\n",
        "\n",
        "modelRelevance.fit(\n",
        "    [train_input_ids3, train_input_masks3, train_segment_ids3], \n",
        "    train_labels3,\n",
        "    validation_data=([test_input_ids3, test_input_masks3, test_segment_ids3], test_labels3),\n",
        "    epochs=1,\n",
        "    batch_size=32\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer (BertLayer)          (None, 768)          178565115   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            514         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 178,762,493\n",
            "Trainable params: 6,102,530\n",
            "Non-trainable params: 172,659,963\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "10000/10000 [==============================] - 496s 50ms/sample - loss: 0.0834 - acc: 0.9728 - val_loss: 0.0807 - val_acc: 0.9730\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4027161b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai70W2Wj6JuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.models.save_model(modelRelevance,'relevance.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyTmpKH76MRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del modelRelevance\n",
        "sess.close()\n",
        "K.clear_session()\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9t_rK_GN0Sf",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoEFbAx2YGYY",
        "colab_type": "text"
      },
      "source": [
        "The three models are used together to evaluate the evidences collected from google."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDnC4JqmWs9E",
        "colab_type": "code",
        "outputId": "58ecb606-c348-4831-b1ca-cc6df697474a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Tokenizer is initialized in case that the previous code was not run\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh9dr_-WOO7i",
        "colab_type": "code",
        "outputId": "66a0ae64-3be1-4961-8d2f-9aafe3a7f203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "# The three different models are loaded\n",
        "modelWorthy = tf.keras.models.load_model('worthy.h5',custom_objects={'BertLayer': BertLayer})\n",
        "modelRelevance = tf.keras.models.load_model('relevance.h5',custom_objects={'BertLayer': BertLayer})\n",
        "modelEvidences = tf.keras.models.load_model('evidences.h5',custom_objects={'BertLayer': BertLayer})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0bKf6-qOaeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A couple of paragraphs of a speech from Donald Trump\n",
        "speech = \"Madam President, Mr. Secretary-General, world leaders, ambassadors, and distinguished delegates: One year ago, I stood before you for the first time in this grand hall. I addressed the threats facing our world, and I presented a vision to achieve a brighter future for all of humanity. Today, I stand before the United Nations General Assembly to share the extraordinary progress we’ve made. In less than two years, my administration has accomplished more than almost any administration in the history of our country. Didn’t expect that reaction, but that’s okay. (Laughter and applause.) America’s economy is booming like never before. Since my election, we’ve added $10 trillion in wealth. The stock market is at an all-time high in history, and jobless claims are at a 50-year low. African American, Hispanic American, and Asian American unemployment have all achieved their lowest levels ever recorded. We’ve added more than 4 million new jobs, including half a million manufacturing jobs. We have passed the biggest tax cuts and reforms in American history. We’ve started the construction of a major border wall, and we have greatly strengthened border security. We have secured record funding for our military — $700 billion this year, and $716 billion next year. Our military will soon be more powerful than it has ever been before. In other words, the United States is stronger, safer, and a richer country than it was when I assumed office less than two years ago. We are standing up for America and for the American people. And we are also standing up for the world. This is great news for our citizens and for peace-loving people everywhere. We believe that when nations respect the rights of their neighbors, and defend the interests of their people, they can better work together to secure the blessings of safety, prosperity, and peace. Each of us here today is the emissary of a distinct culture, a rich history, and a people bound together by ties of memory, tradition, and the values that make our homelands like nowhere else on Earth. That is why America will always choose independence and cooperation over global governance, control, and domination. I honor the right of every nation in this room to pursue its own customs, beliefs, and traditions. The United States will not tell you how to live or work or worship. We only ask that you honor our sovereignty in return.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKwEDTX3OhQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenization into sentences\n",
        "sentences = sent_tokenize(speech)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clmaF5ksPVpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each sentence is evaluated for check-worthiness and ranked\n",
        "ranked = {}\n",
        "\n",
        "for sentence in sentences:\n",
        "  test_examples = convert_text_to_examples([sentence], [0])\n",
        "  (test_input_ids, test_input_masks, test_segment_ids, test_labels) = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)\n",
        "\n",
        "  pred = modelWorthy.predict([test_input_ids, test_input_masks, test_segment_ids])\n",
        "  \n",
        "  ranked[sentence]= pred[0][1]\n",
        "  \n",
        "import operator\n",
        "sortedRank = sorted(ranked.items(), key=operator.itemgetter(1),reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzvk4E28SRI-",
        "colab_type": "code",
        "outputId": "801b2836-3676-4e3c-fda8-0f46b9275e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Each claim is used as a query for google, importance is obtained based on relevance of results, supportive/confuting stance of evidences is evaluated. \n",
        "# 3 articles are evaluated for each claim.\n",
        "proofsNumber = 3\n",
        "\n",
        "# Only the top 15% of claims are considered\n",
        "for sentence in sortedRank[:(int(0.15*len(sortedRank)))]:\n",
        "  sentence = sentence[0]\n",
        "  print(\"\\n--> Sentence:\",sentence)\n",
        "  proofsFound = 0\n",
        "  query = sentence\n",
        "  query = re.sub('\"', ' ', query)\n",
        "  \n",
        "  for link in search(query, tld=\"co.uk\", num=50, start=0, pause=2): \n",
        "    pdf=0\n",
        "    print(\"Analyzed Link: \",link)\n",
        "      \n",
        "    try:\n",
        "      response = requests.get(link, timeout=5)\n",
        "    except:\n",
        "      continue\n",
        "      \n",
        "    content_type = response.headers.get('content-type')\n",
        "\n",
        "    try:\n",
        "      if 'application/pdf' in content_type:\n",
        "        continue\n",
        "        pdf=1\n",
        "    except:\n",
        "      continue\n",
        "    \n",
        "    if(pdf==1):\n",
        "      try:\n",
        "        f = io.BytesIO(response.content)\n",
        "        reader = PdfFileReader(f)\n",
        "        summary = reader.getPage(0).extractText().split('\\n')\n",
        "        summary2 = reader.getPage(1).extractText().split('\\n')\n",
        "        summary3 = reader.getPage(2).extractText().split('\\n')\n",
        "        summary = str(summary)\n",
        "        summary2 = str(summary2)\n",
        "        summary3 = str(summary3)\n",
        "        summary = summary + summary2+ summary3\n",
        "      except:\n",
        "        continue\n",
        "    else:\n",
        "      try:\n",
        "        response = requests.get(link, timeout=10)\n",
        "      except:\n",
        "        continue\n",
        "      try:\n",
        "        doc = Document(response.text)\n",
        "      except:\n",
        "        continue\n",
        "      try:\n",
        "        summary = doc.summary()\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "    body = re.sub('<.*?>', ' ', summary)\n",
        "    body = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', body, flags=re.MULTILINE)\n",
        "    body = re.sub('&#13;', ' ', body)\n",
        "    body = re.sub('[^A-Za-z0-9.,!? ]+', ' ', body)\n",
        "    body = re.sub(' +', ' ', body)\n",
        "\n",
        "    print(\"Article Content is:\",body[:100],\"...\")\n",
        "    if(len(body)<250):\n",
        "      continue\n",
        "    \n",
        "    if(body=='' or body==' ' or body.startswith(\" 403\") or body.startswith(\" Forbidden\") or body.startswith(\" 404\")):\n",
        "      continue\n",
        "    \n",
        "    test_examples = convert_text_to_examples_m([sentence],[body], [0])\n",
        "    (test_input_ids, test_input_masks, test_segment_ids, test_labels) = convert_examples_to_features_m(tokenizer, test_examples, max_seq_length=max_seq_length)\n",
        "\n",
        "    relevance = modelRelevance.predict([test_input_ids, test_input_masks, test_segment_ids])\n",
        "    \n",
        "    stance = modelEvidences.predict([test_input_ids, test_input_masks, test_segment_ids])\n",
        "   \n",
        "    \n",
        "    if(relevance[0][1]<0.5):\n",
        "      print(\"--> Document Not Relevant\")\n",
        "      continue\n",
        "    else:\n",
        "      print(\"--> Document Is Relevant\")\n",
        "      \n",
        "    print(\"--> Importance:\",relevance[0][1])\n",
        "    \n",
        "    print(\"Likelihood to be supporting:\",stance[0][1])\n",
        "    \n",
        "    if(stance[0][1]>0.5):\n",
        "      print(\"--> Document is Supporting\")\n",
        "    else:\n",
        "      print(\"--> Document is not Supporting\")\n",
        "\n",
        "    proofsFound += 1\n",
        "    \n",
        "    if(proofsFound>=proofsNumber):\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--> Sentence: The United States will not tell you how to live or work or worship.\n",
            "Analyzed Link:  https://www.youtube.com/watch?v=esUMI1wnFhU\n",
            "Article Content is:  Rating is available when the video has been rented. This feature is not available right now. Please ...\n",
            "Analyzed Link:  https://www.cbsnews.com/live-news/donald-trump-un-speech-united-nations-general-assembly-address-today-live-updates-2018-09-25/\n",
            "Article Content is:  Trump touts America first policies at U.N. General Assembly President Trump excoriated the ideology ...\n",
            "--> Document Is Relevant\n",
            "--> Importance: 0.7908329\n",
            "Likelihood to be supporting: 0.47012958\n",
            "--> Document is not Supporting\n",
            "Analyzed Link:  https://www.whitehouse.gov/briefings-statements/remarks-president-trump-73rd-session-united-nations-general-assembly-new-york-ny/\n",
            "Article Content is:  United Nations Headquarters New York, New York 10 38 A.M. EDT THE PRESIDENT Madam President, Mr. Se ...\n",
            "--> Document Is Relevant\n",
            "--> Importance: 0.9274754\n",
            "Likelihood to be supporting: 0.480015\n",
            "--> Document is not Supporting\n",
            "Analyzed Link:  https://www.washingtonpost.com/politics/at-united-nations-trump-rejects-constraints-imposed-by-other-nations-says-us-will-act-to-counter-global-control/2018/09/25/1a5f8b22-c0c9-11e8-be77-516336a26305_story.html\n",
            "Analyzed Link:  https://twitter.com/whitehouse/status/1044610844582191104?lang=en\n",
            "Article Content is:  Welcome home! This timeline is where you ll spend most of your time, getting instant updates about  ...\n",
            "--> Document Is Relevant\n",
            "--> Importance: 0.93166506\n",
            "Likelihood to be supporting: 0.47113317\n",
            "--> Document is not Supporting\n",
            "\n",
            "--> Sentence: And we are also standing up for the world.\n",
            "Analyzed Link:  https://www.whitehouse.gov/briefings-statements/remarks-president-trump-73rd-session-united-nations-general-assembly-new-york-ny/\n",
            "Article Content is:  United Nations Headquarters New York, New York 10 38 A.M. EDT THE PRESIDENT Madam President, Mr. Se ...\n",
            "--> Document Is Relevant\n",
            "--> Importance: 0.7672437\n",
            "Likelihood to be supporting: 0.4764431\n",
            "--> Document is not Supporting\n",
            "Analyzed Link:  https://www.whitehouse.gov/briefings-statements/remarks-president-trump-press-conference-3/\n",
            "Article Content is:  InterContinental New York Barclay New York, New York 4 28 P.M. EDT PRESIDENT TRUMP Thank you very m ...\n",
            "--> Document Is Relevant\n",
            "--> Importance: 0.7911169\n",
            "Likelihood to be supporting: 0.47914687\n",
            "--> Document is not Supporting\n",
            "Analyzed Link:  https://www.hrw.org/sites/default/files/supporting_resources/hrw_world_report_2019_etr_0.pdf\n",
            "Analyzed Link:  https://www.cnn.com/2018/09/25/politics/donald-trump-un-speech/index.html\n",
            "Article Content is:  Many of the assembled world leaders and foreign dignitaries responded with laughter. I didn t expec ...\n",
            "--> Document Is Relevant\n",
            "--> Importance: 0.91512305\n",
            "Likelihood to be supporting: 0.47320998\n",
            "--> Document is not Supporting\n",
            "\n",
            "--> Sentence: We have secured record funding for our military — $700 billion this year, and $716 billion next year.\n",
            "Analyzed Link:  http://www.msnbc.com/rachel-maddow-show/when-does-trump-believe-current-military-spending-crazy\n",
            "Article Content is:  A few months ago, when Bob Woodward s latest book was published, Donald Trump s White House team se ...\n",
            "--> Document Is Relevant\n",
            "--> Importance: 0.60607207\n",
            "Likelihood to be supporting: 0.5163345\n",
            "--> Document is Supporting\n",
            "Analyzed Link:  https://www.whitehouse.gov/briefings-statements/remarks-president-trump-73rd-session-united-nations-general-assembly-new-york-ny/\n",
            "Article Content is:  United Nations Headquarters New York, New York 10 38 A.M. EDT THE PRESIDENT Madam President, Mr. Se ...\n",
            "--> Document Is Relevant\n",
            "--> Importance: 0.74450827\n",
            "Likelihood to be supporting: 0.49849194\n",
            "--> Document is not Supporting\n",
            "Analyzed Link:  https://www.whitehouse.gov/trump-administration-accomplishments/\n",
            "Article Content is:  Trump Administration Accomplishments  ...\n",
            "Analyzed Link:  https://checkyourfact.com/2018/09/29/fact-check-700-billion-military-budget-record/\n",
            "Article Content is:  President Donald Trump attended a rally in Missouri recently where he claimed that Congress had pas ...\n",
            "--> Document Is Relevant\n",
            "--> Importance: 0.75545806\n",
            "Likelihood to be supporting: 0.5189786\n",
            "--> Document is Supporting\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}